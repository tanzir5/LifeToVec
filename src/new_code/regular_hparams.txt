vocab_size: 15822 #this one you should knwo in advance (fix later)
batch_size: 10
max_length: 100
hidden_size: 280
hidden_ff: 2210
hidden_act: "swish"
n_encoders: 5
n_heads: 10
n_local: 7
local_window_size: 38
norm_type: "rezero"
att_dropout: 0.1 ## Attention Layers
fw_dropout: 0.1 ## Positionwise Layers
dc_dropout: 0.1 ## Decoder Layer
emb_dropout: 0.1 ## Embedding dropout
parametrize_emb: True
norm_input_emb: False
norm_output_emb: True
weight_tying: "wt"
## TASK AND LOGS
training_task: mlm # name of the task [mlm, simple]
experiment_name: pretrain_dutch_0
experiment_version: 1
## ATTENTION
attention_type: "performer"
multihead_dc: False
num_random_features: 436

# From trainer args..
learning_rate: 5.e-3
weight_decay: 0.01
beta1: 0.9
beta2: 0.999

cls_num_targs: 3

epsilon: 1.e-6
stage: "pre_training"
implementation: "v1"
version: 1.0
